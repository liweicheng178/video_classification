{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_datasetBreakfast import load_data, read_mapping_dict\n",
    "import os  \n",
    "import torch\n",
    "import numpy as np\n",
    "import os.path \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isArrayLike(obj):\n",
    "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
    "\n",
    " \n",
    "def load_data(split_load, actions_dict, GT_folder, DATA_folder, datatype = 'training',):\n",
    "    file_ptr = open(split_load, 'r')\n",
    "    content_all = file_ptr.read().split('\\n')[1:-1]\n",
    "    content_all = [x.strip('./data/groundTruth/') + 't' for x in content_all]\n",
    "    all_tasks = ['tea', 'cereals', 'coffee', 'friedegg', 'juice', 'milk', 'sandwich', 'scrambledegg', 'pancake', 'salat']\n",
    "    \n",
    "    if datatype == 'training':\n",
    "        data_breakfast = []\n",
    "        labels_breakfast = []\n",
    "        for content in content_all:\n",
    "        \n",
    "            file_ptr = open( GT_folder + content, 'r')\n",
    "            curr_gt = file_ptr.read().split('\\n')[:-1]\n",
    "            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
    "            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
    "            label_curr_video = []\n",
    "            for iik in range(len(curr_gt)):\n",
    "                label_curr_video.append( actions_dict[curr_gt[iik]] )\n",
    "         \n",
    "            data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
    "            labels_breakfast.append(label_curr_video )\n",
    "    \n",
    "        labels_uniq, labels_uniq_loc = get_label_bounds(labels_breakfast)\n",
    "        print(\"Finish Load the Training data and labels!!!\")     \n",
    "        return  data_breakfast, labels_uniq\n",
    "    if datatype == 'test':\n",
    "        data_breakfast = []\n",
    "        \n",
    "        segment = []\n",
    "        for content in content_all:\n",
    "            \n",
    "            #file_ptr = open( GT_folder + content, 'r')\n",
    "            #curr_gt = file_ptr.read().split('\\n')[:-1]\n",
    "            \n",
    "            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
    "        \n",
    "            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
    "                                 \n",
    "            data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
    "            \n",
    "        \n",
    "        return data_breakfast\n",
    "\n",
    "\n",
    "def get_label_bounds( data_labels):\n",
    "    labels_uniq = []\n",
    "    labels_uniq_loc = []\n",
    "    for kki in range(0, len(data_labels) ):\n",
    "        uniq_group, indc_group = get_label_length_seq(data_labels[kki])\n",
    "        labels_uniq.append(uniq_group[1:-1])\n",
    "        labels_uniq_loc.append(indc_group[1:-1])\n",
    "    return labels_uniq, labels_uniq_loc\n",
    "\n",
    "def get_label_length_seq(content):\n",
    "    label_seq = []\n",
    "    length_seq = []\n",
    "    start = 0\n",
    "    length_seq.append(0)\n",
    "    for i in range(len(content)):\n",
    "        if content[i] != content[start]:\n",
    "            #print(content[i])\n",
    "            label_seq.append(content[start])\n",
    "            length_seq.append(i)\n",
    "            start = i\n",
    "    label_seq.append(content[start])\n",
    "    length_seq.append(len(content))\n",
    "    \n",
    "    if content[-1] != 0:\n",
    "        label_seq.append(content[-1])\n",
    "    \n",
    "    return label_seq, length_seq\n",
    "\n",
    "\n",
    "def get_maxpool_lstm_data(cData, indices):\n",
    "    list_data = []\n",
    "    for kkl in range(len(indices)-1):\n",
    "        cur_start = indices[kkl]\n",
    "        cur_end = indices[kkl+1]\n",
    "        if cur_end > cur_start:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "        else:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end+1,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "    list_data  =  torch.stack(list_data)\n",
    "    return list_data\n",
    "\n",
    "def read_mapping_dict(mapping_file):\n",
    "    file_ptr = open(mapping_file, 'r')\n",
    "    actions = file_ptr.read().split('\\n')[:-1]\n",
    "\n",
    "    actions_dict=dict()\n",
    "    for a in actions:\n",
    "        actions_dict[a.split()[1]] = int(a.split()[0])\n",
    "\n",
    "    return actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMP_PATH = ''\n",
    "train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle') #Train Split\n",
    "test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle') #Test Split\n",
    "GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/') #Ground Truth Labels for each training video \n",
    "DATA_folder =  os.path.join(COMP_PATH, 'data/') #Frame I3D features for all videos\n",
    "mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_dict = read_mapping_dict(mapping_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feat = load_data( test_split, actions_dict, GT_folder, DATA_folder, datatype = 'test') #Get features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_feature.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(data_feat,  'test_feature.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Load the Training data and labels!!!\n"
     ]
    }
   ],
   "source": [
    "train_feat, train_labels = load_data( train_split, actions_dict, GT_folder, DATA_folder, datatype = 'training') #Get features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_feature.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(train_feat,  'train_feature.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_label.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(train_labels,  'train_label.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
